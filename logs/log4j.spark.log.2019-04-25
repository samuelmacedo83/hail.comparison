19/04/25 14:08:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/25 14:08:29 INFO SparkContext: Running Spark version 2.4.0
19/04/25 14:08:29 INFO SparkContext: Submitted application: sparklyr
19/04/25 14:08:29 INFO SecurityManager: Changing view acls to: samuel
19/04/25 14:08:29 INFO SecurityManager: Changing modify acls to: samuel
19/04/25 14:08:29 INFO SecurityManager: Changing view acls groups to: 
19/04/25 14:08:29 INFO SecurityManager: Changing modify acls groups to: 
19/04/25 14:08:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(samuel); groups with view permissions: Set(); users  with modify permissions: Set(samuel); groups with modify permissions: Set()
19/04/25 14:08:29 INFO Utils: Successfully started service 'sparkDriver' on port 36265.
19/04/25 14:08:29 INFO SparkEnv: Registering MapOutputTracker
19/04/25 14:08:29 INFO SparkEnv: Registering BlockManagerMaster
19/04/25 14:08:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/25 14:08:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/25 14:08:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e4566efc-f8ea-41f5-9537-3cf4a0260d56
19/04/25 14:08:29 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
19/04/25 14:08:29 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/25 14:08:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/25 14:08:29 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
19/04/25 14:08:29 INFO SparkContext: Added JAR file:/home/samuel/R/x86_64-pc-linux-gnu-library/3.5/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:36265/jars/sparklyr-2.3-2.11.jar with timestamp 1556212109642
19/04/25 14:08:29 INFO Executor: Starting executor ID driver on host localhost
19/04/25 14:08:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40271.
19/04/25 14:08:29 INFO NettyBlockTransferService: Server created on localhost:40271
19/04/25 14:08:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/25 14:08:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 40271, None)
19/04/25 14:08:29 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40271 with 912.3 MB RAM, BlockManagerId(driver, localhost, 40271, None)
19/04/25 14:08:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 40271, None)
19/04/25 14:08:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 40271, None)
19/04/25 14:08:32 INFO SharedState: loading hive config file: file:/home/samuel/spark/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml
19/04/25 14:08:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse').
19/04/25 14:08:32 INFO SharedState: Warehouse path is 'file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse'.
19/04/25 14:08:33 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/04/25 14:08:35 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/04/25 14:08:35 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/04/25 14:08:35 INFO ObjectStore: ObjectStore, initialize called
19/04/25 14:08:36 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/04/25 14:08:36 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/04/25 14:08:37 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/04/25 14:08:38 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:08:38 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:08:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:08:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:08:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/04/25 14:08:39 INFO ObjectStore: Initialized ObjectStore
19/04/25 14:08:39 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/04/25 14:08:39 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/04/25 14:08:39 INFO HiveMetaStore: Added admin role in metastore
19/04/25 14:08:39 INFO HiveMetaStore: Added public role in metastore
19/04/25 14:08:39 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/04/25 14:08:39 INFO HiveMetaStore: 0: get_all_databases
19/04/25 14:08:39 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_all_databases	
19/04/25 14:08:39 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/04/25 14:08:39 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/04/25 14:08:39 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:08:40 INFO SessionState: Created local directory: /tmp/6d03282c-6288-41a1-8d02-61a25974dbbb_resources
19/04/25 14:08:40 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/6d03282c-6288-41a1-8d02-61a25974dbbb
19/04/25 14:08:40 INFO SessionState: Created local directory: /tmp/samuel/6d03282c-6288-41a1-8d02-61a25974dbbb
19/04/25 14:08:40 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/6d03282c-6288-41a1-8d02-61a25974dbbb/_tmp_space.db
19/04/25 14:08:40 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse
19/04/25 14:08:40 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:08:40 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:08:40 INFO HiveMetaStore: 0: get_database: global_temp
19/04/25 14:08:40 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/04/25 14:08:40 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/04/25 14:08:40 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:08:40 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:08:40 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:08:40 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:08:40 INFO HiveMetaStore: 0: get_tables: db=default pat=*
19/04/25 14:08:40 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/04/25 14:08:42 INFO CodeGenerator: Code generated in 179.299435 ms
19/04/25 14:29:35 INFO SparkContext: Invoking stop() from shutdown hook
19/04/25 14:29:35 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
19/04/25 14:29:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/25 14:29:35 INFO MemoryStore: MemoryStore cleared
19/04/25 14:29:35 INFO BlockManager: BlockManager stopped
19/04/25 14:29:35 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/25 14:29:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/25 14:29:35 INFO SparkContext: Successfully stopped SparkContext
19/04/25 14:29:35 INFO ShutdownHookManager: Shutdown hook called
19/04/25 14:29:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-c94938c7-295a-4687-9841-8021d51547f9
19/04/25 14:29:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-aea2bce7-3d23-4513-8fb6-56ba8ee7976d
19/04/25 14:40:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/25 14:40:10 INFO SparkContext: Running Spark version 2.4.0
19/04/25 14:40:10 INFO SparkContext: Submitted application: sparklyr
19/04/25 14:40:10 INFO SecurityManager: Changing view acls to: samuel
19/04/25 14:40:10 INFO SecurityManager: Changing modify acls to: samuel
19/04/25 14:40:10 INFO SecurityManager: Changing view acls groups to: 
19/04/25 14:40:10 INFO SecurityManager: Changing modify acls groups to: 
19/04/25 14:40:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(samuel); groups with view permissions: Set(); users  with modify permissions: Set(samuel); groups with modify permissions: Set()
19/04/25 14:40:10 INFO Utils: Successfully started service 'sparkDriver' on port 42577.
19/04/25 14:40:10 INFO SparkEnv: Registering MapOutputTracker
19/04/25 14:40:10 INFO SparkEnv: Registering BlockManagerMaster
19/04/25 14:40:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/25 14:40:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/25 14:40:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-246cecad-b062-4de0-b0a1-f0d02820b424
19/04/25 14:40:10 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
19/04/25 14:40:10 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/25 14:40:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/25 14:40:10 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
19/04/25 14:40:10 INFO SparkContext: Added JAR file:/home/samuel/R/x86_64-pc-linux-gnu-library/3.5/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:42577/jars/sparklyr-2.3-2.11.jar with timestamp 1556214010897
19/04/25 14:40:10 INFO Executor: Starting executor ID driver on host localhost
19/04/25 14:40:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36867.
19/04/25 14:40:11 INFO NettyBlockTransferService: Server created on localhost:36867
19/04/25 14:40:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/25 14:40:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 36867, None)
19/04/25 14:40:11 INFO BlockManagerMasterEndpoint: Registering block manager localhost:36867 with 912.3 MB RAM, BlockManagerId(driver, localhost, 36867, None)
19/04/25 14:40:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 36867, None)
19/04/25 14:40:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 36867, None)
19/04/25 14:40:15 INFO SharedState: loading hive config file: file:/home/samuel/spark/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml
19/04/25 14:40:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse').
19/04/25 14:40:15 INFO SharedState: Warehouse path is 'file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse'.
19/04/25 14:40:15 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/04/25 14:40:17 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/04/25 14:40:17 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/04/25 14:40:17 INFO ObjectStore: ObjectStore, initialize called
19/04/25 14:40:18 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/04/25 14:40:18 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/04/25 14:40:19 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/04/25 14:40:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:40:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:40:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:40:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:40:21 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/04/25 14:40:21 INFO ObjectStore: Initialized ObjectStore
19/04/25 14:40:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/04/25 14:40:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/04/25 14:40:21 INFO HiveMetaStore: Added admin role in metastore
19/04/25 14:40:21 INFO HiveMetaStore: Added public role in metastore
19/04/25 14:40:21 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_all_databases
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_all_databases	
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/04/25 14:40:21 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:40:21 INFO SessionState: Created local directory: /tmp/1502d530-9e65-449f-864b-172651880a4e_resources
19/04/25 14:40:21 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/1502d530-9e65-449f-864b-172651880a4e
19/04/25 14:40:21 INFO SessionState: Created local directory: /tmp/samuel/1502d530-9e65-449f-864b-172651880a4e
19/04/25 14:40:21 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/1502d530-9e65-449f-864b-172651880a4e/_tmp_space.db
19/04/25 14:40:21 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_database: global_temp
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/04/25 14:40:21 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:40:21 INFO HiveMetaStore: 0: get_tables: db=default pat=*
19/04/25 14:40:21 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/04/25 14:40:22 INFO CodeGenerator: Code generated in 161.237323 ms
19/04/25 14:53:56 INFO SparkContext: Invoking stop() from shutdown hook
19/04/25 14:53:56 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
19/04/25 14:53:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/25 14:53:56 INFO MemoryStore: MemoryStore cleared
19/04/25 14:53:56 INFO BlockManager: BlockManager stopped
19/04/25 14:53:56 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/25 14:53:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/25 14:53:56 INFO SparkContext: Successfully stopped SparkContext
19/04/25 14:53:56 INFO ShutdownHookManager: Shutdown hook called
19/04/25 14:53:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-a8b91a6c-c96e-4c29-b3a1-e8ea02763e0e
19/04/25 14:53:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ebac414-84a9-461b-97ca-e34def71ef2b
19/04/25 14:54:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/25 14:54:31 INFO SparkContext: Running Spark version 2.4.0
19/04/25 14:54:31 INFO SparkContext: Submitted application: sparklyr
19/04/25 14:54:31 INFO SecurityManager: Changing view acls to: samuel
19/04/25 14:54:31 INFO SecurityManager: Changing modify acls to: samuel
19/04/25 14:54:31 INFO SecurityManager: Changing view acls groups to: 
19/04/25 14:54:31 INFO SecurityManager: Changing modify acls groups to: 
19/04/25 14:54:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(samuel); groups with view permissions: Set(); users  with modify permissions: Set(samuel); groups with modify permissions: Set()
19/04/25 14:54:31 INFO Utils: Successfully started service 'sparkDriver' on port 38129.
19/04/25 14:54:31 INFO SparkEnv: Registering MapOutputTracker
19/04/25 14:54:31 INFO SparkEnv: Registering BlockManagerMaster
19/04/25 14:54:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/25 14:54:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/25 14:54:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dcbfa178-f226-4bb1-bbac-3b3c54efbc58
19/04/25 14:54:31 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
19/04/25 14:54:31 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/25 14:54:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/25 14:54:31 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
19/04/25 14:54:31 INFO SparkContext: Added JAR file:/home/samuel/R/x86_64-pc-linux-gnu-library/3.5/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:38129/jars/sparklyr-2.3-2.11.jar with timestamp 1556214871497
19/04/25 14:54:31 INFO Executor: Starting executor ID driver on host localhost
19/04/25 14:54:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39241.
19/04/25 14:54:31 INFO NettyBlockTransferService: Server created on localhost:39241
19/04/25 14:54:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/25 14:54:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 39241, None)
19/04/25 14:54:31 INFO BlockManagerMasterEndpoint: Registering block manager localhost:39241 with 912.3 MB RAM, BlockManagerId(driver, localhost, 39241, None)
19/04/25 14:54:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 39241, None)
19/04/25 14:54:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 39241, None)
19/04/25 14:54:35 INFO SharedState: loading hive config file: file:/home/samuel/spark/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml
19/04/25 14:54:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse').
19/04/25 14:54:35 INFO SharedState: Warehouse path is 'file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse'.
19/04/25 14:54:36 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/04/25 14:54:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/04/25 14:54:38 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/04/25 14:54:38 INFO ObjectStore: ObjectStore, initialize called
19/04/25 14:54:38 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/04/25 14:54:38 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/04/25 14:54:40 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/04/25 14:54:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:54:40 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:54:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:54:41 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:54:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/04/25 14:54:41 INFO ObjectStore: Initialized ObjectStore
19/04/25 14:54:41 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/04/25 14:54:41 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/04/25 14:54:42 INFO HiveMetaStore: Added admin role in metastore
19/04/25 14:54:42 INFO HiveMetaStore: Added public role in metastore
19/04/25 14:54:42 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_all_databases
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_all_databases	
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/04/25 14:54:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:54:42 INFO SessionState: Created local directory: /tmp/5160e3d9-d4da-4643-a961-cfc3d2dad09f_resources
19/04/25 14:54:42 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/5160e3d9-d4da-4643-a961-cfc3d2dad09f
19/04/25 14:54:42 INFO SessionState: Created local directory: /tmp/samuel/5160e3d9-d4da-4643-a961-cfc3d2dad09f
19/04/25 14:54:42 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/5160e3d9-d4da-4643-a961-cfc3d2dad09f/_tmp_space.db
19/04/25 14:54:42 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_database: global_temp
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/04/25 14:54:42 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:54:42 INFO HiveMetaStore: 0: get_tables: db=default pat=*
19/04/25 14:54:42 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/04/25 14:54:43 INFO CodeGenerator: Code generated in 183.386405 ms
19/04/25 14:56:04 INFO SparkContext: Invoking stop() from shutdown hook
19/04/25 14:56:04 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
19/04/25 14:56:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/25 14:56:04 INFO MemoryStore: MemoryStore cleared
19/04/25 14:56:04 INFO BlockManager: BlockManager stopped
19/04/25 14:56:04 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/25 14:56:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/25 14:56:04 INFO SparkContext: Successfully stopped SparkContext
19/04/25 14:56:04 INFO ShutdownHookManager: Shutdown hook called
19/04/25 14:56:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-af4ffb08-6870-4390-9e38-42de82fc7a11
19/04/25 14:56:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c14107e-d62c-44c6-b16e-ed62ed396354
19/04/25 14:57:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/25 14:57:40 INFO SparkContext: Running Spark version 2.4.0
19/04/25 14:57:40 INFO SparkContext: Submitted application: sparklyr
19/04/25 14:57:40 INFO SecurityManager: Changing view acls to: samuel
19/04/25 14:57:40 INFO SecurityManager: Changing modify acls to: samuel
19/04/25 14:57:40 INFO SecurityManager: Changing view acls groups to: 
19/04/25 14:57:40 INFO SecurityManager: Changing modify acls groups to: 
19/04/25 14:57:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(samuel); groups with view permissions: Set(); users  with modify permissions: Set(samuel); groups with modify permissions: Set()
19/04/25 14:57:40 INFO Utils: Successfully started service 'sparkDriver' on port 46531.
19/04/25 14:57:40 INFO SparkEnv: Registering MapOutputTracker
19/04/25 14:57:40 INFO SparkEnv: Registering BlockManagerMaster
19/04/25 14:57:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/25 14:57:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/25 14:57:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1eda46dd-cbe6-478d-9c3d-754d16e08174
19/04/25 14:57:40 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
19/04/25 14:57:40 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/25 14:57:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/25 14:57:40 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
19/04/25 14:57:40 INFO SparkContext: Added JAR file:///home/samuel/R/x86_64-pc-linux-gnu-library/3.5/hail.comparison/java/hail.comparison-2.4-2.11.jar at spark://localhost:46531/jars/hail.comparison-2.4-2.11.jar with timestamp 1556215060909
19/04/25 14:57:40 INFO SparkContext: Added JAR file:/home/samuel/R/x86_64-pc-linux-gnu-library/3.5/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:46531/jars/sparklyr-2.3-2.11.jar with timestamp 1556215060911
19/04/25 14:57:41 INFO Executor: Starting executor ID driver on host localhost
19/04/25 14:57:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46427.
19/04/25 14:57:41 INFO NettyBlockTransferService: Server created on localhost:46427
19/04/25 14:57:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/25 14:57:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 46427, None)
19/04/25 14:57:41 INFO BlockManagerMasterEndpoint: Registering block manager localhost:46427 with 912.3 MB RAM, BlockManagerId(driver, localhost, 46427, None)
19/04/25 14:57:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 46427, None)
19/04/25 14:57:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 46427, None)
19/04/25 14:57:45 INFO SharedState: loading hive config file: file:/home/samuel/spark/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml
19/04/25 14:57:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse').
19/04/25 14:57:45 INFO SharedState: Warehouse path is 'file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse'.
19/04/25 14:57:45 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/04/25 14:57:46 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/04/25 14:57:47 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/04/25 14:57:47 INFO ObjectStore: ObjectStore, initialize called
19/04/25 14:57:47 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/04/25 14:57:47 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/04/25 14:57:49 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/04/25 14:57:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:57:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:57:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:57:50 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:57:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/04/25 14:57:50 INFO ObjectStore: Initialized ObjectStore
19/04/25 14:57:51 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/04/25 14:57:51 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/04/25 14:57:51 INFO HiveMetaStore: Added admin role in metastore
19/04/25 14:57:51 INFO HiveMetaStore: Added public role in metastore
19/04/25 14:57:51 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_all_databases
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_all_databases	
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/04/25 14:57:51 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 14:57:51 INFO SessionState: Created local directory: /tmp/4d0e59be-ea8c-4c0f-ad2f-43d0c8b20155_resources
19/04/25 14:57:51 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/4d0e59be-ea8c-4c0f-ad2f-43d0c8b20155
19/04/25 14:57:51 INFO SessionState: Created local directory: /tmp/samuel/4d0e59be-ea8c-4c0f-ad2f-43d0c8b20155
19/04/25 14:57:51 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/4d0e59be-ea8c-4c0f-ad2f-43d0c8b20155/_tmp_space.db
19/04/25 14:57:51 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_database: global_temp
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/04/25 14:57:51 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_database: default
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 14:57:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
19/04/25 14:57:51 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/04/25 14:57:52 INFO CodeGenerator: Code generated in 164.123696 ms
19/04/25 15:00:34 INFO SparkContext: Invoking stop() from shutdown hook
19/04/25 15:00:34 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
19/04/25 15:00:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/25 15:00:34 INFO MemoryStore: MemoryStore cleared
19/04/25 15:00:34 INFO BlockManager: BlockManager stopped
19/04/25 15:00:34 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/25 15:00:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/25 15:00:34 INFO SparkContext: Successfully stopped SparkContext
19/04/25 15:00:34 INFO ShutdownHookManager: Shutdown hook called
19/04/25 15:00:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff5f8c37-b4e2-4d87-be03-23ad3c4ab68a
19/04/25 15:00:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-3b1faef7-10bf-42c2-9634-12ff769e0934
19/04/25 15:00:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/04/25 15:00:46 INFO SparkContext: Running Spark version 2.4.0
19/04/25 15:00:46 INFO SparkContext: Submitted application: sparklyr
19/04/25 15:00:46 INFO SecurityManager: Changing view acls to: samuel
19/04/25 15:00:46 INFO SecurityManager: Changing modify acls to: samuel
19/04/25 15:00:46 INFO SecurityManager: Changing view acls groups to: 
19/04/25 15:00:46 INFO SecurityManager: Changing modify acls groups to: 
19/04/25 15:00:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(samuel); groups with view permissions: Set(); users  with modify permissions: Set(samuel); groups with modify permissions: Set()
19/04/25 15:00:47 INFO Utils: Successfully started service 'sparkDriver' on port 46241.
19/04/25 15:00:47 INFO SparkEnv: Registering MapOutputTracker
19/04/25 15:00:47 INFO SparkEnv: Registering BlockManagerMaster
19/04/25 15:00:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/04/25 15:00:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/04/25 15:00:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2837e497-4901-42c0-b2c6-4e3b94aa3db1
19/04/25 15:00:47 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
19/04/25 15:00:47 INFO SparkEnv: Registering OutputCommitCoordinator
19/04/25 15:00:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/04/25 15:00:47 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
19/04/25 15:00:47 INFO SparkContext: Added JAR file:///home/samuel/R/x86_64-pc-linux-gnu-library/3.5/hail.comparison/java/hail.comparison-2.4-2.11.jar at spark://localhost:46241/jars/hail.comparison-2.4-2.11.jar with timestamp 1556215247492
19/04/25 15:00:47 INFO SparkContext: Added JAR file:///home/samuel/R/x86_64-pc-linux-gnu-library/3.5/hail.comparison/java/variant-spark_2.11-0.2.0-a1.jar at spark://localhost:46241/jars/variant-spark_2.11-0.2.0-a1.jar with timestamp 1556215247493
19/04/25 15:00:47 INFO SparkContext: Added JAR file:/home/samuel/R/x86_64-pc-linux-gnu-library/3.5/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:46241/jars/sparklyr-2.3-2.11.jar with timestamp 1556215247493
19/04/25 15:00:47 INFO Executor: Starting executor ID driver on host localhost
19/04/25 15:00:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43797.
19/04/25 15:00:47 INFO NettyBlockTransferService: Server created on localhost:43797
19/04/25 15:00:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/04/25 15:00:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 43797, None)
19/04/25 15:00:47 INFO BlockManagerMasterEndpoint: Registering block manager localhost:43797 with 912.3 MB RAM, BlockManagerId(driver, localhost, 43797, None)
19/04/25 15:00:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 43797, None)
19/04/25 15:00:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 43797, None)
19/04/25 15:00:51 INFO SharedState: loading hive config file: file:/home/samuel/spark/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml
19/04/25 15:00:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse').
19/04/25 15:00:51 INFO SharedState: Warehouse path is 'file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse'.
19/04/25 15:00:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/04/25 15:00:53 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/04/25 15:00:54 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/04/25 15:00:54 INFO ObjectStore: ObjectStore, initialize called
19/04/25 15:00:54 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/04/25 15:00:54 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/04/25 15:00:55 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/04/25 15:00:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 15:00:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 15:00:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 15:00:57 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 15:00:57 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/04/25 15:00:57 INFO ObjectStore: Initialized ObjectStore
19/04/25 15:00:57 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/04/25 15:00:57 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/04/25 15:00:58 INFO HiveMetaStore: Added admin role in metastore
19/04/25 15:00:58 INFO HiveMetaStore: Added public role in metastore
19/04/25 15:00:58 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_all_databases
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_all_databases	
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/04/25 15:00:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/04/25 15:00:58 INFO SessionState: Created local directory: /tmp/f5ecc73c-ce70-40d9-ad6c-8f756bf8a084_resources
19/04/25 15:00:58 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/f5ecc73c-ce70-40d9-ad6c-8f756bf8a084
19/04/25 15:00:58 INFO SessionState: Created local directory: /tmp/samuel/f5ecc73c-ce70-40d9-ad6c-8f756bf8a084
19/04/25 15:00:58 INFO SessionState: Created HDFS directory: /tmp/hive/samuel/f5ecc73c-ce70-40d9-ad6c-8f756bf8a084/_tmp_space.db
19/04/25 15:00:58 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/samuel/Pesquisa/hail.comparison/spark-warehouse
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_database: default
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_database: global_temp
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: global_temp	
19/04/25 15:00:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_database: default
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_database: default
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_database: default	
19/04/25 15:00:58 INFO HiveMetaStore: 0: get_tables: db=default pat=*
19/04/25 15:00:58 INFO audit: ugi=samuel	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
19/04/25 15:00:59 INFO CodeGenerator: Code generated in 183.888082 ms
19/04/25 16:10:54 INFO SparkContext: Invoking stop() from shutdown hook
19/04/25 16:10:54 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
19/04/25 16:10:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/04/25 16:10:54 INFO MemoryStore: MemoryStore cleared
19/04/25 16:10:54 INFO BlockManager: BlockManager stopped
19/04/25 16:10:54 INFO BlockManagerMaster: BlockManagerMaster stopped
19/04/25 16:10:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/04/25 16:10:54 INFO SparkContext: Successfully stopped SparkContext
19/04/25 16:10:54 INFO ShutdownHookManager: Shutdown hook called
19/04/25 16:10:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-5f65d558-72ec-4a16-8c34-bb277a90a7f1
19/04/25 16:10:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-a94671a9-0c8c-469a-b5bf-e69409f513e2
